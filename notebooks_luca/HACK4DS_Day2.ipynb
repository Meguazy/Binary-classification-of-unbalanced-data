{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/HACK_4DS/blob/main/notebooks_luca/HACK4DS_Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qmD21Xwaismg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c455c699-014e-40bd-d72d-c1d56eae9ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HACK_4DS'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 102 (delta 42), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 11.92 MiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Meguazy/HACK_4DS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3IsSBfmCDzn",
        "outputId": "c327a403-a96c-4f8c-c050-84de60ed3d0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/HACK_4DS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_R2sqZ4kkX2",
        "outputId": "5b90b7e5-fd39-48b9-92e4-3234958beda7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HACK_4DS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial"
      ],
      "metadata": {
        "id": "-Ha47nwWoTsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHTpD5nQLGP3",
        "outputId": "2d919598-22ed-49cd-fe65-641bcd3c2ad6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import lightgbm as lgb\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_iDiHJkQkVce"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"data2/test_set.csv\", sep=\";\", quotechar=\"\\\"\", decimal=\",\")"
      ],
      "metadata": {
        "id": "rqCRCcm2kwbj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"data2/train_set.csv\", sep=\";\", quotechar=\"\\\"\", decimal=\",\")"
      ],
      "metadata": {
        "id": "FD8F2dIckg19"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "tgyeiRX7epi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_score_ver03_dictionary = {\n",
        "    \"MISSING\":None,\n",
        "    \"A\":14,\n",
        "    \"B\":13,\n",
        "    \"C\":12,\n",
        "    \"D\":11,\n",
        "    \"E\":10,\n",
        "    \"F\":9,\n",
        "    \"G\":8,\n",
        "    \"H\":7,\n",
        "    \"I\":6,\n",
        "    \"L\":5,\n",
        "    \"M\":4,\n",
        "    \"N\":3,\n",
        "    \"O\":2,\n",
        "    \"P\":1\n",
        "}\n",
        "\n",
        "df_train = df_train.replace({\"external_score_ver03\": external_score_ver03_dictionary})\n",
        "df_test = df_test.replace({\"external_score_ver03\": external_score_ver03_dictionary})"
      ],
      "metadata": {
        "id": "NMttmmMtUojp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate df_train and df_test\n",
        "df_combined = pd.concat([df_train, df_test], axis=0)\n",
        "\n",
        "# List of columns to encode\n",
        "columns_to_encode = ['province', 'juridical_form', 'industry_sector', 'region', 'geo_area', 'decision_date']\n",
        "\n",
        "# Instantiate LabelEncoder\n",
        "label_encoders = {}\n",
        "\n",
        "# Encode each column iteratively\n",
        "for column in columns_to_encode:\n",
        "    # Instantiate LabelEncoder for the current column\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "\n",
        "    # Fit label encoder on combined data\n",
        "    label_encoders[column].fit(df_combined[column])\n",
        "\n",
        "    # Transform both training and testing data\n",
        "    df_train[column] = label_encoders[column].transform(df_train[column])\n",
        "    df_test[column] = label_encoders[column].transform(df_test[column])"
      ],
      "metadata": {
        "id": "_NkmKUtUYcWR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_columns = ['decision_date', 'company_ID', 'external_score_ver01',\n",
        "       'external_score_ver02', 'late_payment_score',\n",
        "       'external_score_late_payment_integrated', 'external_score_moderate',\n",
        "       'external_score_adverse', 'external_score_ver03', 'age', 'province',\n",
        "       'juridical_form', 'industry_sector', 'gross_margin_ratio',\n",
        "       'core_income_ratio', 'cash_asset_ratio',\n",
        "       'consolidated_liabilities_ratio', 'tangible_assets_ratio', 'revenues',\n",
        "       'cr_available', 'region', 'geo_area', 'last_statement_age',\n",
        "       'overrun_freq_a_revoca_autoliquidanti',\n",
        "       'avg_tension_a_revoca_autoliquidanti',\n",
        "       'std_tension_a_revoca_autoliquidanti',\n",
        "       'max_tension_a_revoca_autoliquidanti',\n",
        "       'last_tension_a_revoca_autoliquidanti',\n",
        "       'avg_rel_used_a_revoca_autoliquidanti',\n",
        "       'std_rel_used_a_revoca_autoliquidanti',\n",
        "       'max_rel_used_a_revoca_autoliquidanti',\n",
        "       'last_rel_used_a_revoca_autoliquidanti', 'overrun_freq_a_scadenza',\n",
        "       'avg_rel_used_a_scadenza', 'std_rel_used_a_scadenza',\n",
        "       'max_rel_used_a_scadenza', 'last_rel_used_a_scadenza',\n",
        "       'avg_count_enti_affidanti', 'std_count_enti_affidanti',\n",
        "       'max_count_enti_affidanti', 'last_count_enti_affidanti',\n",
        "       'avg_count_numero_prima_info', 'std_count_numero_prima_info',\n",
        "       'max_count_numero_prima_info', 'last_count_numero_prima_info']\n",
        "\n",
        "reg_columns = ['decision_date', 'company_ID', 'external_score_ver01',\n",
        "       'external_score_ver02', 'external_score_ver03', 'age', 'province',\n",
        "       'juridical_form', 'industry_sector', 'gross_margin_ratio',\n",
        "       'core_income_ratio', 'cash_asset_ratio',\n",
        "       'consolidated_liabilities_ratio', 'tangible_assets_ratio', 'revenues',\n",
        "       'cr_available', 'region', 'geo_area', 'last_statement_age',\n",
        "       'overrun_freq_a_revoca_autoliquidanti',\n",
        "       'avg_tension_a_revoca_autoliquidanti',\n",
        "       'std_tension_a_revoca_autoliquidanti',\n",
        "       'max_tension_a_revoca_autoliquidanti',\n",
        "       'last_tension_a_revoca_autoliquidanti',\n",
        "       'avg_rel_used_a_revoca_autoliquidanti',\n",
        "       'std_rel_used_a_revoca_autoliquidanti',\n",
        "       'max_rel_used_a_revoca_autoliquidanti',\n",
        "       'last_rel_used_a_revoca_autoliquidanti', 'overrun_freq_a_scadenza',\n",
        "       'avg_rel_used_a_scadenza', 'std_rel_used_a_scadenza',\n",
        "       'max_rel_used_a_scadenza', 'last_rel_used_a_scadenza',\n",
        "       'avg_count_enti_affidanti', 'std_count_enti_affidanti',\n",
        "       'max_count_enti_affidanti', 'last_count_enti_affidanti',\n",
        "       'avg_count_numero_prima_info', 'std_count_numero_prima_info',\n",
        "       'max_count_numero_prima_info', 'last_count_numero_prima_info']"
      ],
      "metadata": {
        "id": "rmHlg-SQRbsi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificator"
      ],
      "metadata": {
        "id": "CRybM-5OMZUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import lightgbm as lgb\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(df_train[class_columns], df_train.target, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = lgb.Dataset(X_train_class, label=y_train_class)\n",
        "\n",
        "\n",
        "# Setting up parameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error',\n",
        "    'learning_rate': 0.05,\n",
        "    'random_state': 42,\n",
        "    'is_unbalance': True\n",
        "}\n",
        "\n",
        "# Training the model\n",
        "num_round = 3100\n",
        "bst = lgb.train(params, train_data, num_round)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_class = bst.predict(X_test_class, num_iteration=bst.best_iteration)\n",
        "\n",
        "# Converting probabilities to binary predictions\n",
        "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_class]\n",
        "\n",
        "# Calculating F1 score\n",
        "f1 = f1_score(y_test_class, y_pred_binary)\n",
        "print(\"F1 score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XczGqIAlMpgA",
        "outputId": "9797db12-ad62-4f46-c463-1384d6b9536e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 6893, number of negative: 25135\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6189\n",
            "[LightGBM] [Info] Number of data points in the train set: 32028, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.215218 -> initscore=-1.293755\n",
            "[LightGBM] [Info] Start training from score -1.293755\n",
            "F1 score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressor"
      ],
      "metadata": {
        "id": "fKcF-6RSMqhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(df_train, df_train.days_to_default, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XNuJetfttOoq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Scaling the features\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# X_train_scaled = scaler.fit_transform(X_train_reg[reg_columns])\n",
        "# X_test_scaled = scaler.transform(X_test_reg[reg_columns])\n",
        "# df_test_scaled = scaler.transform(df_test[reg_columns])\n",
        "X_train_scaled = X_train_reg[reg_columns]\n",
        "X_test_scaled = X_test_reg[reg_columns]\n",
        "df_test_scaled = df_test[reg_columns]"
      ],
      "metadata": {
        "id": "-Fh-SoinADIN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "model = LGBMRegressor(\n",
        "    objective='regression',\n",
        "    boosting_type='gbdt',\n",
        "    num_leaves=51,\n",
        "    learning_rate=0.18,\n",
        "    n_estimators=200,\n",
        "    feature_fraction=0.88,\n",
        "    bagging_fraction=0.8,\n",
        "    num_iterations=6000,\n",
        "    bagging_freq=5,\n",
        "    verbose=-1,\n",
        ")\n",
        "model.fit(X_train_scaled, y_train_reg)\n",
        "\n",
        "# Make predictions on the training and validation data.\n",
        "y_train_pred_reg = model.predict(X_train_scaled)\n",
        "y_pred_reg = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the Root Mean Absolute Error (MAE) for training and validation predictions.\n",
        "print(\"Training MAE: \", mae(y_train_reg, y_train_pred_reg))\n",
        "print(\"Validation MAE: \", mae(y_test_reg, y_pred_reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beytlfczO1wk",
        "outputId": "f2b38dba-9753-4db3-dd4e-b367cff86f02"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE:  0.2361893075122995\n",
            "Validation MAE:  244.61924728545512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My regressor"
      ],
      "metadata": {
        "id": "_V6pQpcHPnj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n",
        "#                               AdaBoostRegressor)\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "# from sklearn.model_selection import KFold, cross_val_score\n",
        "# import numpy as np\n",
        "\n",
        "# Model = []\n",
        "# MAE = []\n",
        "# RMSE = []\n",
        "# R_sq = []\n",
        "# cv = KFold(5)\n",
        "\n",
        "# #Creating a Function to append the cross validation scores of the algorithms\n",
        "# def input_scores(name, model, x, y):\n",
        "#     Model.append(name)\n",
        "#     MAE.append((-1) * cross_val_score(model, x, y, cv=cv,\n",
        "#                                       scoring='neg_mean_absolute_error').mean())\n",
        "#     RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=cv,\n",
        "#                                                scoring='neg_mean_squared_error').mean()))\n",
        "#     R_sq.append(cross_val_score(model, x, y, cv=cv, scoring='r2').mean())\n",
        "\n",
        "# names = ['K Neighbors Regressor', 'Decision Tree Regressor',\n",
        "#          'Random Forest Regressor', 'Gradient Boosting Regressor', 'LightGBM']\n",
        "\n",
        "\n",
        "# models = [\n",
        "#     KNeighborsRegressor(),\n",
        "#     DecisionTreeRegressor(),\n",
        "#     RandomForestRegressor(),\n",
        "#     GradientBoostingRegressor(),\n",
        "#     LGBMRegressor(\n",
        "#         objective='regression',\n",
        "#         metric='mae',\n",
        "#         boosting_type='gbdt',\n",
        "#         num_leaves=51,\n",
        "#         learning_rate=0.18,\n",
        "#         n_estimators=200,\n",
        "#         feature_fraction=0.88,\n",
        "#         begging_fraction=0.8,\n",
        "#         bagging_freq=5,\n",
        "#         verbose=-1\n",
        "#     )\n",
        "# ]\n",
        "\n",
        "# # Impute missing values\n",
        "# imputer = SimpleImputer(strategy='median')  # You can change the strategy as needed\n",
        "# X_train_imputed = imputer.fit_transform(X_train_scaled)"
      ],
      "metadata": {
        "id": "SMh6CqROtYRI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Running all algorithms\n",
        "# for name, model in zip(names, models):\n",
        "#     input_scores(name, model, X_train_imputed, y_train)\n",
        "\n",
        "# evaluation = pd.DataFrame({'Model': Model,\n",
        "#                            'MAE' : MAE,\n",
        "#                            'RMSE': RMSE,\n",
        "#                            'R Squared': R_sq})\n",
        "# print(\"FOLLOWING ARE THE TRAINING SCORES: \")\n",
        "# evaluation"
      ],
      "metadata": {
        "id": "G7ljNDQ7_u5M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize and train the LGBMRegressor\n",
        "# lgbm_regressor = LGBMRegressor(\n",
        "#     objective='regression',\n",
        "#     metric='mae',\n",
        "#     boosting_type='gbdt',\n",
        "#     num_leaves=51,\n",
        "#     learning_rate=0.18,\n",
        "#     n_estimators=200,\n",
        "#     feature_fraction=0.88,\n",
        "#     begging_fraction=0.8,\n",
        "#     bagging_freq=5,\n",
        "#     verbose=-1\n",
        "# )\n",
        "\n",
        "# lgbm_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Predict on the testing set\n",
        "# y_pred = lgbm_regressor.predict(X_test_scaled)\n",
        "\n",
        "# # Calculate Mean Absolute Error (MAE)\n",
        "# mae = mean_absolute_error(y_test, y_pred)\n",
        "# print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "nzKXd-90y_SK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Validation"
      ],
      "metadata": {
        "id": "ZxIEKsh2Pi6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_local_val = X_test_reg.copy()\n",
        "\n",
        "# Making predictions\n",
        "y_class_local = bst.predict(df_local_val[class_columns], num_iteration=bst.best_iteration)\n",
        "\n",
        "# Converting probabilities to binary predictions\n",
        "y_class_local_binary = [1 if pred > 0.5 else 0 for pred in y_class_local]\n",
        "\n",
        "df_local_val['default_prediction'] = y_class_local_binary"
      ],
      "metadata": {
        "id": "q4bluHe2Pkzn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_local_val['reg_prediction'] = df_local_val.apply(lambda row: model.predict([row[reg_columns].values])[0] if row['default_prediction'] == 1 else 1498, axis=1)"
      ],
      "metadata": {
        "id": "_ZAjLq5XX7WB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_local_val"
      ],
      "metadata": {
        "id": "H6SRGXupYfO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation Local MAE: \", mae(df_local_val.days_to_default, df_local_val.reg_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16goa0Fba23A",
        "outputId": "fa51cfec-862f-42a2-cde5-40e844712666"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Local MAE:  112.3372738003468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit solution"
      ],
      "metadata": {
        "id": "DEA6Y5Wz3IkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_val = df_test.copy()\n",
        "\n",
        "# Making predictions\n",
        "y_final = bst.predict(df_test_val[class_columns], num_iteration=bst.best_iteration)\n",
        "\n",
        "# Converting probabilities to binary predictions\n",
        "y_final_binary = [1 if pred > 0.5 else 0 for pred in y_final]\n",
        "\n",
        "df_test_val['default_prediction'] = y_final_binary\n",
        "\n",
        "df_test_val['reg_prediction'] = df_test_val.apply(lambda row: model.predict([row[reg_columns].values])[0] if row['default_prediction'] == 1 else 1498, axis=1)"
      ],
      "metadata": {
        "id": "cpLof21jbPWC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(df_test_val['reg_prediction'])"
      ],
      "metadata": {
        "id": "e2Iz0L2Yrv8r"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"file.csv\", index=False, header=False)"
      ],
      "metadata": {
        "id": "RE2SBQXzr9uh"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}